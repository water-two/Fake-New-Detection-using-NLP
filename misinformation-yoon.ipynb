{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":10798626,"sourceType":"datasetVersion","datasetId":6702150}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Cell 1 – environment setup\nimport os, sys, subprocess, random, logging, requests\nimport numpy as np, pandas as pd, torch, matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import (accuracy_score, precision_score,\n                             recall_score, f1_score, classification_report)\n\ndef _require(mod_name: str, pip_name: str | None = None, attr: str | None = None):\n    \"\"\"\n    Import `mod_name`; if missing, pip-install `pip_name` (or the same name) then import.\n    Optionally return a symbol inside the module (`attr`).\n    \"\"\"\n    import importlib\n    try:\n        mod = importlib.import_module(mod_name)\n    except ModuleNotFoundError:\n        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-qU\",\n                               pip_name or mod_name])\n        mod = importlib.import_module(mod_name)\n    return getattr(mod, attr) if attr else mod\n\nBM25Okapi = _require(\"rank_bm25\", attr=\"BM25Okapi\")   \n_require(\"bert_score\", \"bert-score\")                  \n\nlogging.basicConfig(level=logging.INFO,\n                    format=\"%(asctime)s | %(levelname)-7s | %(message)s\",\n                    datefmt=\"%H:%M:%S\")\nlogger = logging.getLogger(__name__)\n\nSEED = 42\nrandom.seed(SEED); np.random.seed(SEED); torch.manual_seed(SEED)\nif torch.cuda.is_available():\n    torch.cuda.manual_seed(SEED)\n    torch.backends.cudnn.deterministic, torch.backends.cudnn.benchmark = True, False\n\nFAST_DEV = bool(int(os.getenv(\"FAST_DEV\", \"1\")))\nFAST_SAMPLE_TRAIN, FAST_SAMPLE_TEST, FAST_EPOCHS = 2_000, 500, 2\nEPOCHS   = FAST_EPOCHS if FAST_DEV else 3\n\nDATA_PATH   = \"/kaggle/input/dataset/\"\nMODEL_NAME  = \"bert-base-uncased\"\nBATCH_SIZE  = 16\nMAX_LENGTH  = 512\nLEARNING_RATE = 2e-5\nTEST_SIZE  = 0.30\nDEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nlogger.info(f\"Device: {DEVICE} | FAST_DEV: {FAST_DEV}\")\n\nfrom kaggle_secrets import UserSecretsClient\nFACT_CHECK_API_KEY = UserSecretsClient().get_secret(\"FACT_CHECK_API_KEY\")\n\ndef query_fact_check_api(claim: str, page_size: int = 5) -> dict | None:\n    url = \"https://factchecktools.googleapis.com/v1/claims:search\"\n    params = {\"query\": claim, \"languageCode\": \"en\",\n              \"pageSize\": page_size, \"key\": FACT_CHECK_API_KEY}\n    try:\n        r = requests.get(url, params=params, timeout=10)\n        r.raise_for_status()\n        return r.json()\n    except requests.RequestException as e:\n        logger.warning(f\"Fact-Check API error: {e}\")\n        return None\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-11T08:27:52.482648Z","iopub.execute_input":"2025-06-11T08:27:52.482950Z","iopub.status.idle":"2025-06-11T08:27:52.635079Z","shell.execute_reply.started":"2025-06-11T08:27:52.482929Z","shell.execute_reply":"2025-06-11T08:27:52.634434Z"}},"outputs":[],"execution_count":84},{"cell_type":"code","source":"import os, json, pandas as pd\nfrom sklearn.model_selection import train_test_split\n\nLABEL2ID      = {\"SUPPORTS\": 0, \"REFUTES\": 1, \"NOT ENOUGH INFO\": 2}\nFEVER_LABELS  = list(LABEL2ID)\nFEVER_CLASSES = set(FEVER_LABELS)\n\ndef _map_to_fever(lbl: str):\n    mapping = {\n        # LIAR & FakeNewsNet\n        \"real\": \"SUPPORTS\", \"fake\": \"REFUTES\",\n        \"false\": \"REFUTES\", \"pants_on_fire\": \"REFUTES\", \"barely_true\": \"REFUTES\",\n        \"half_true\": \"SUPPORTS\", \"mostly_true\": \"SUPPORTS\", \"true\": \"SUPPORTS\",\n        # already-FEVER\n        \"SUPPORTS\": \"SUPPORTS\", \"REFUTES\": \"REFUTES\", \"NOT ENOUGH INFO\": \"NOT ENOUGH INFO\",\n    }\n    return mapping.get(str(lbl).strip())\n\ndef _read_jsonl(fp):\n    with open(fp, encoding=\"utf-8\") as f:\n        return [json.loads(line) for line in f]\n\ndef load_and_preprocess_data(path: str,\n                             logger=None,\n                             fast_dev: bool = False,\n                             fast_train: int = 2000,\n                             fast_test:  int = 500):\n    # ---------- LIAR ----------\n    liar_cols = [\"id\", \"label\", \"statement\", \"subject\", \"speaker\", \"job\", \"state\",\n                 \"party\", \"barely_true\", \"false\", \"half_true\", \"mostly_true\",\n                 \"pants_on_fire\", \"context\"]\n\n    def _load_liar(split):\n        df = pd.read_csv(os.path.join(path, f\"{split}.tsv\"), sep=\"\\t\", header=None)\n        df.columns = liar_cols\n        df[\"label\"] = df[\"label\"].apply(_map_to_fever)\n        df[\"source\"] = \"liar\"\n        return df[[\"statement\", \"label\", \"source\"]]\n\n    liar_df = pd.concat([_load_liar(s) for s in (\"train\", \"valid\", \"test\")])\n\n    # ---------- FEVER ----------\n    def _load_fever(split):\n        df = pd.DataFrame(_read_jsonl(os.path.join(path, f\"{split}.jsonl\")))\n        df = df[[\"claim\", \"label\"]].rename(columns={\"claim\": \"statement\"})\n        df[\"label\"] = df[\"label\"].apply(_map_to_fever)\n        df[\"source\"] = \"fever\"\n        return df\n\n    fever_df = pd.concat([_load_fever(s) for s in (\"train\", \"paper_dev\", \"paper_test\")])\n\n    # ---------- FakeNewsNet ----------\n    def _load_fnn(csv_file, real_fake, src):\n        df = pd.read_csv(os.path.join(path, csv_file))\n        text_col = \"title\" if \"title\" in df.columns else \"content\" if \"content\" in df.columns else None\n        if text_col is None:\n            raise ValueError(f\"No text column in {csv_file}\")\n        df = df.rename(columns={text_col: \"statement\"})\n        df[\"label\"] = _map_to_fever(real_fake)\n        df[\"source\"] = src\n        return df[[\"statement\", \"label\", \"source\"]]\n\n    fnn_df = pd.concat([\n        _load_fnn(\"politifact_real.csv\",  \"real\", \"politifact\"),\n        _load_fnn(\"politifact_fake.csv\",  \"fake\", \"politifact\"),\n        _load_fnn(\"gossipcop_real.csv\",   \"real\", \"gossipcop\"),\n        _load_fnn(\"gossipcop_fake.csv\",   \"fake\", \"gossipcop\"),\n    ])\n\n    # ---------- combine & clean ----------\n    data = pd.concat([liar_df, fever_df, fnn_df], ignore_index=True)\n    data.dropna(subset=[\"statement\", \"label\"], inplace=True)\n    data[\"statement\"] = data[\"statement\"].astype(str).str.strip()\n    data = data[data[\"statement\"].str.len() > 10]\n    data = data.drop_duplicates(subset=[\"statement\", \"label\"])\n    data = data[data[\"label\"].isin(FEVER_CLASSES)].reset_index(drop=True)\n\n    if logger:\n        logger.info(\"Label distribution:\\n%s\", data[\"label\"].value_counts())\n\n    train_df, test_df = train_test_split(\n        data, test_size=0.30, stratify=data[\"label\"], random_state=42\n    )\n\n    if fast_dev:\n        def _subsample(df, n):\n            return df.groupby(\"label\", group_keys=False)\\\n                     .apply(lambda x: x.sample(min(n, len(x)), random_state=42))\n        train_df = _subsample(train_df, fast_train // 3).reset_index(drop=True)\n        test_df  = _subsample(test_df,  fast_test  // 3).reset_index(drop=True)\n        if logger:\n            logger.info(f\"FAST_DEV: train {len(train_df)}, test {len(test_df)}\")\n\n    return train_df.reset_index(drop=True), test_df.reset_index(drop=True)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-11T08:27:52.636157Z","iopub.execute_input":"2025-06-11T08:27:52.636406Z","iopub.status.idle":"2025-06-11T08:27:52.648907Z","shell.execute_reply.started":"2025-06-11T08:27:52.636389Z","shell.execute_reply":"2025-06-11T08:27:52.648176Z"}},"outputs":[],"execution_count":85},{"cell_type":"code","source":"# Cell 3 – dataset, dataloaders, CNN (BERT evidence-ready)\nimport torch, torch.nn as nn, torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\nimport pandas as pd          # needed inside Dataset\n\nLABEL2ID  = {\"SUPPORTS\": 0, \"REFUTES\": 1, \"NOT ENOUGH INFO\": 2}\nID2LABEL  = {v: k for k, v in LABEL2ID.items()}\n\n# ── Dataset ──────────────────────────────────────────────────────────\nclass FeverDataset(Dataset):\n    \"\"\"\n    If use_evidence=True the tokenizer is fed (claim, evidence) pairs.\n    Evidence is taken from:\n      • pre-computed df[\"evidence\"] OR\n      • evidence_fn(claim)  – if supplied\n    \"\"\"\n    def __init__(self, df, tokenizer, max_len=512,\n                 use_evidence=False, evidence_fn=None):\n        self.df   = df.reset_index(drop=True)\n        self.tok  = tokenizer\n        self.max  = max_len\n        self.ev   = use_evidence\n        self.evfn = evidence_fn\n\n    def __len__(self): return len(self.df)\n\n    def __getitem__(self, idx):\n        row   = self.df.iloc[idx]\n        claim = str(row[\"statement\"])\n        label = LABEL2ID.get(row[\"label\"], 2)\n\n        if self.ev:                                   # claim + evidence mode\n            evidence = (row[\"evidence\"]\n                        if \"evidence\" in row and pd.notna(row[\"evidence\"])\n                        else (self.evfn(claim) if self.evfn else \"\"))\n            enc = self.tok(\n                claim, evidence,\n                max_length=self.max, truncation=True,\n                padding=\"max_length\", return_tensors=\"pt\"\n            )\n        else:                                         # claim-only mode\n            enc = self.tok(\n                claim,\n                max_length=self.max, truncation=True,\n                padding=\"max_length\", return_tensors=\"pt\"\n            )\n\n        return {\n            \"input_ids\":      enc[\"input_ids\"].squeeze(0),\n            \"attention_mask\": enc[\"attention_mask\"].squeeze(0),\n            \"label\":          torch.tensor(label)\n        }\n\n# ── Dataloaders ─────────────────────────────────────────────────────\ndef create_dataloaders(train_df, test_df, tok,\n                       bs=16, max_len=512,\n                       use_evidence=False, evidence_fn=None,\n                       workers=2, pin=True):\n    defs = dict(tokenizer=tok, max_len=max_len,\n                use_evidence=use_evidence, evidence_fn=evidence_fn)\n    mk   = lambda df: FeverDataset(df, **defs)\n    mkdl = lambda ds, shuf: DataLoader(ds, bs, shuffle=shuf,\n                                       num_workers=workers, pin_memory=pin)\n    return mkdl(mk(train_df), True), mkdl(mk(test_df), False)\n\n# ── CNN (unchanged except default dropout 0.3) ─────────────────────\nclass CNNModel(nn.Module):\n    def __init__(self, vocab_size, embed_dim=256, classes=3,\n                 dropout=0.3, pretrained_weights=None, freeze_embed=False):\n        super().__init__()\n        self.embed = nn.Embedding(vocab_size, embed_dim, padding_idx=0)\n        if pretrained_weights is not None:\n            self.embed.weight.data[: pretrained_weights.size(0)] = pretrained_weights\n            if freeze_embed:\n                self.embed.weight.requires_grad = False\n\n        ks = (3, 4, 5)\n        self.convs = nn.ModuleList([\n            nn.Sequential(\n                nn.Conv2d(1, 100, (k, embed_dim), padding=(k-1, 0)),\n                nn.ReLU(),\n                nn.BatchNorm2d(100, momentum=0.05)  # quicker BN updates\n            ) for k in ks\n        ])\n        self.drop = nn.Dropout(dropout)\n        self.fc   = nn.Linear(100 * len(ks), classes)\n\n    def forward(self, ids):\n        x = self.embed(ids).unsqueeze(1)                         # [B,1,L,D]\n        pooled = [F.max_pool1d(c(x).squeeze(3), c(x).size(2)).squeeze(2)\n                  for c in self.convs]                           # [B,100] × 3\n        return self.fc(self.drop(torch.cat(pooled, 1)))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-11T08:27:52.649679Z","iopub.execute_input":"2025-06-11T08:27:52.649924Z","iopub.status.idle":"2025-06-11T08:27:52.663727Z","shell.execute_reply.started":"2025-06-11T08:27:52.649902Z","shell.execute_reply":"2025-06-11T08:27:52.663123Z"}},"outputs":[],"execution_count":86},{"cell_type":"code","source":"import numpy as np, torch\nfrom sklearn.metrics import (accuracy_score, precision_score,\n                             recall_score, f1_score, classification_report)\n\nLABEL2ID     = {\"SUPPORTS\":0, \"REFUTES\":1, \"NOT ENOUGH INFO\":2}\nFEVER_LABELS = [\"SUPPORTS\", \"REFUTES\", \"NOT ENOUGH INFO\"]\nDEVICE       = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# ── BERT loader ─────────────────────────────────────────────\ndef get_bert_model(model_name=\"bert-base-uncased\", num_labels=3):\n    from transformers import BertTokenizer, BertForSequenceClassification\n    tok = BertTokenizer.from_pretrained(model_name)\n    mdl = BertForSequenceClassification.from_pretrained(model_name,\n                                                        num_labels=num_labels).to(DEVICE)\n    return tok, mdl\n\n# ── shared helpers ─────────────────────────────────────────\ndef _eval_epoch(model, dl):\n    model.eval(); y_true, y_pred = [], []\n    with torch.no_grad():\n        for b in dl:\n            ids  = b[\"input_ids\"].to(DEVICE)\n            mask = b[\"attention_mask\"].to(DEVICE)\n            labs = b[\"label\"].to(DEVICE)\n            logits = model(ids, attention_mask=mask).logits if hasattr(model,\"config\") else model(ids)\n            y_true.extend(labs.cpu().numpy())\n            y_pred.extend(torch.argmax(logits, 1).cpu().numpy())\n    return np.array(y_true), np.array(y_pred)\n\ndef _report(split, y_true, y_pred):\n    logger.info(f\"{split} acc: {accuracy_score(y_true,y_pred):.3f}\")\n    logger.info(\"\\n\"+classification_report(\n        y_true, y_pred, labels=[0,1,2], target_names=FEVER_LABELS,\n        digits=3, zero_division=0))\n\n# ── BERT training ──────────────────────────────────────────\ndef train_bert_model(model, train_dl, val_dl, optim,\n                     epochs=3, use_amp=False, class_w=None):\n    loss_fn = torch.nn.CrossEntropyLoss(weight=class_w)\n    scaler  = torch.cuda.amp.GradScaler() if use_amp else None\n\n    for ep in range(1, epochs+1):\n        model.train(); total = 0\n        for b in train_dl:\n            optim.zero_grad()\n            ids = b[\"input_ids\"].to(DEVICE); mask = b[\"attention_mask\"].to(DEVICE)\n            labs= b[\"label\"].to(DEVICE)\n            if use_amp:\n                with torch.cuda.amp.autocast():\n                    loss = loss_fn(model(ids,attention_mask=mask).logits, labs)\n                scaler.scale(loss).backward(); scaler.step(optim); scaler.update()\n            else:\n                loss = loss_fn(model(ids,attention_mask=mask).logits, labs)\n                loss.backward(); optim.step()\n            total += loss.item()\n        logger.info(f\"[BERT] epoch {ep} loss {total/len(train_dl):.4f}\")\n        _report(\"val\", *_eval_epoch(model, val_dl))\n\ndef evaluate_bert_model(model, test_dl):\n    y_t, y_p = _eval_epoch(model, test_dl)\n    _report(\"test\", y_t, y_p)\n    return {\"Accuracy\": accuracy_score(y_t,y_p),\n            \"Precision\": precision_score(y_t,y_p,average=\"weighted\",zero_division=0),\n            \"Recall\":    recall_score  (y_t,y_p,average=\"weighted\",zero_division=0),\n            \"F1 Score\":  f1_score      (y_t,y_p,average=\"weighted\",zero_division=0)}\n\n# ── CNN training / eval ───────────────────────────────────\ndef train_cnn_model(model, train_dl, val_dl, optim,\n                    epochs=3, class_w=None):\n    model.to(DEVICE)\n    loss_fn = torch.nn.CrossEntropyLoss(weight=class_w)\n    for ep in range(1, epochs+1):\n        model.train(); total = 0\n        for b in train_dl:\n            optim.zero_grad()\n            ids, labs = b[\"input_ids\"].to(DEVICE), b[\"label\"].to(DEVICE)\n            loss = loss_fn(model(ids), labs)\n            loss.backward(); optim.step()\n            total += loss.item()\n        logger.info(f\"[CNN] epoch {ep} loss {total/len(train_dl):.4f}\")\n        _report(\"val\", *_eval_epoch(model, val_dl))\n\ndef evaluate_cnn_model(model, test_dl):\n    y_t, y_p = _eval_epoch(model, test_dl)\n    _report(\"test\", y_t, y_p)\n    return {\"Accuracy\": accuracy_score(y_t,y_p),\n            \"Precision\": precision_score(y_t,y_p,average=\"weighted\",zero_division=0),\n            \"Recall\":    recall_score  (y_t,y_p,average=\"weighted\",zero_division=0),\n            \"F1 Score\":  f1_score      (y_t,y_p,average=\"weighted\",zero_division=0)}\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-11T08:27:52.665324Z","iopub.execute_input":"2025-06-11T08:27:52.665737Z","iopub.status.idle":"2025-06-11T08:27:52.684307Z","shell.execute_reply.started":"2025-06-11T08:27:52.665721Z","shell.execute_reply":"2025-06-11T08:27:52.683678Z"}},"outputs":[],"execution_count":87},{"cell_type":"code","source":"import os, re, json, time, random, requests, nltk, numpy as np, pandas as pd\nfrom bs4 import BeautifulSoup\nfrom urllib.robotparser import RobotFileParser\nfrom nltk.corpus import stopwords\nfrom nltk.stem import PorterStemmer\nfrom rank_bm25 import BM25Okapi\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report\n\nnltk.download(\"punkt\", quiet=True)\nnltk.download(\"stopwords\", quiet=True)\n\nLABEL2ID = {\"SUPPORTS\": 0, \"REFUTES\": 1, \"NOT ENOUGH INFO\": 2}\nFEVER_LABELS = [\"SUPPORTS\", \"REFUTES\", \"NOT ENOUGH INFO\"]\n\n# ── text prep\n_stop = set(stopwords.words(\"english\"))\n_stem = PorterStemmer()\ndef preprocess(text: str, remove_stop=True, stem=True):\n    toks = nltk.word_tokenize(re.sub(r\"[^\\w\\s]\", \" \", text.lower()))\n    if remove_stop:\n        toks = [t for t in toks if t not in _stop]\n    if stem:\n        toks = [_stem.stem(t) for t in toks]\n    return toks\n\n# ── scraping helpers\nBASE = {\"politifact\":\"https://www.politifact.com\", \"snopes\":\"https://www.snopes.com\"}\nPAGES = {\"politifact\":\"https://www.politifact.com/factchecks/list/?page={}\",\n         \"snopes\":\"https://www.snopes.com/fact-check/page/{}/\"}\nUA = {\"User-Agent\":\"Mozilla/5.0 (FactCheckBot)\"}\n\ndef _allowed(site, url):\n    rp = RobotFileParser(); rp.set_url(BASE[site] + \"/robots.txt\")\n    try: rp.read(); return rp.can_fetch(\"FactCheckBot\", url)\n    except: return False\n\ndef _html(site, page, retries=3):\n    url = PAGES[site].format(page)\n    if not _allowed(site, url): return None\n    for i in range(retries):\n        try:\n            r = requests.get(url, timeout=10, headers=UA); r.raise_for_status(); return r.text\n        except requests.RequestException: time.sleep(2**i)\n    return None\n\ndef _parse(html, site):\n    soup = BeautifulSoup(html, \"html.parser\"); out=[]\n    if site==\"politifact\":\n        for li in soup.select(\"li.o-listicle__item\"):\n            a = li.select_one(\"a.m-statement__quote\");  img = li.select_one(\"div.m-statement__meter img\")\n            if a: out.append({\"claim\":a.get_text(strip=True),\n                              \"label\":\"REFUTES\" if img and \"false\" in img[\"alt\"].lower() else \"SUPPORTS\"})\n    else:\n        for art in soup.select(\"article.media-block\"):\n            a = art.select_one(\"h2.media-block__title a\"); \n            lbl = art.select_one(\"span.media-rating__link\")\n            if a: out.append({\"claim\":a.get_text(strip=True),\n                              \"label\":\"REFUTES\" if lbl and \"false\" in lbl.get_text(strip=True).lower() else \"SUPPORTS\"})\n    return out\n\ndef scrape_claims(site, pages=2, delay=(1,3)):\n    collected=[]\n    for p in range(1, pages+1):\n        html=_html(site,p); \n        if not html: break\n        claims=_parse(html,site)\n        if not claims: break\n        collected.extend(claims)\n        time.sleep(random.uniform(*delay))\n    return collected\n\n# ── retrieval\ndef retrieve_evidence(claim, bm25, corpus_df, top_n=5):\n    scores = bm25.get_scores(preprocess(claim))\n    idxs = np.argsort(scores)[::-1][:top_n]\n    hits = [(corpus_df.iloc[i][\"label\"], 1/(rank+1)) for rank,i in enumerate(idxs) if scores[i]>0]\n    if not hits:                                        # fall back to web\n        for site in (\"politifact\",\"snopes\"):\n            scrapes = scrape_claims(site, pages=1)\n            if scrapes:\n                hits = [(c[\"label\"], 0.2) for c in scrapes[:top_n]]\n                break\n    return hits\n\ndef majority_vote(hits):\n    if not hits: return \"NOT ENOUGH INFO\"\n    score = {lab:0 for lab in FEVER_LABELS}\n    for lab, w in hits: score[lab]+=w\n    return max(score, key=score.get)\n\n# ── BM25 tuning\ndef tune_bm25(train_df, val_texts, val_labels, tune=False):\n    if not tune: return 1.5, 0.75\n    best_k1, best_b, best = 1.5, 0.75, 0\n    tokens = [preprocess(t) for t in train_df[\"statement\"]]\n    for k1 in [1.2,1.5,1.8,2.0]:\n        for b in [0.6,0.75,0.9]:\n            bm25 = BM25Okapi(tokens, k1=k1, b=b)\n            preds = [majority_vote(retrieve_evidence(t,bm25,train_df)) for t in val_texts]\n            acc = accuracy_score(val_labels, preds)\n            if acc>best: best_k1, best_b, best= k1,b,acc\n    logger.info(f\"BM25 tuned: k1={best_k1}, b={best_b}, acc={best:.3f}\")\n    return best_k1, best_b\n\n# ── evaluation\ndef evaluate_bm25(bm25, texts, labels, train_df, top_n=5):\n    preds=[majority_vote(retrieve_evidence(t,bm25,train_df,top_n)) for t in texts]\n    logger.info(\"\\n\"+classification_report(labels, preds, labels=[0,1,2], target_names=FEVER_LABELS, zero_division=0))\n    return {\"Accuracy\": accuracy_score(labels,preds),\n            \"Precision\": precision_score(labels,preds,average=\"weighted\",zero_division=0),\n            \"Recall\": recall_score(labels,preds,average=\"weighted\",zero_division=0),\n            \"F1 Score\": f1_score(labels,preds,average=\"weighted\",zero_division=0)}\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-11T08:27:52.684955Z","iopub.execute_input":"2025-06-11T08:27:52.685148Z","iopub.status.idle":"2025-06-11T08:27:52.703592Z","shell.execute_reply.started":"2025-06-11T08:27:52.685133Z","shell.execute_reply":"2025-06-11T08:27:52.702841Z"}},"outputs":[],"execution_count":88},{"cell_type":"code","source":"# Cell 6 – master pipeline (BERT sees evidence, 3 epochs)\n\nimport os, numpy as np, pandas as pd, torch, matplotlib.pyplot as plt\nfrom torch.optim import AdamW, Adam\nfrom rank_bm25 import BM25Okapi\n\ntry:\n    import bert_score\nexcept ImportError:\n    os.system(\"pip install -q bert-score\"); import bert_score\n\n# ── wipe checkpoints only if retraining requested\nfor f in (\"bert_model.pt\", \"cnn_model.pt\"):\n    if os.path.exists(f):\n        os.remove(f); print(f\"Deleted old {f}\")\n\ndef plot_metrics(m, title, show=False):\n    if show:\n        plt.barh(list(m.keys()), list(m.values()), color=\"steelblue\")\n        plt.xlim(0, 1); plt.title(title); plt.show()\n\n# ── tiny BM25 helper for evidence\ndef build_bm25(df):\n    return BM25Okapi([preprocess(s) for s in df[\"statement\"]])\n\ndef top_sentence(bm25, corpus_df, claim, min_score=0.30):\n    scores = bm25.get_scores(preprocess(claim))\n    idx    = int(np.argmax(scores))\n    return corpus_df.iloc[idx][\"statement\"] if scores[idx] > min_score else \"\"\n\n# ── main orchestrator\ndef main(force_bm25_tune=False,\n         force_bert_retrain=False,\n         force_cnn_retrain=False,\n         fast_dev=True,\n         show_plots=False):\n\n    logger.info(\"🚀 pipeline start\")\n\n    # data ----------------------------------------------------\n    train_df, test_df = load_and_preprocess_data(\n        DATA_PATH,\n        logger=logger,\n        fast_dev=fast_dev,\n        fast_train=FAST_SAMPLE_TRAIN,\n        fast_test=FAST_SAMPLE_TEST\n    )\n    logger.info(f\"train={len(train_df)} • test={len(test_df)}\")\n\n    # build BM25 for evidence retrieval\n    bm25_ev     = build_bm25(train_df)\n    evidence_fn = lambda c: top_sentence(bm25_ev, train_df, c, 0.30)\n\n    # tokeniser & models\n    tok, bert = get_bert_model(MODEL_NAME, num_labels=3)\n\n    # dataloaders\n    train_dl_bert, test_dl_bert = create_dataloaders(\n        train_df, test_df, tok,\n        bs=BATCH_SIZE, max_len=256,\n        use_evidence=True,  evidence_fn=evidence_fn   # claim + evidence\n    )\n    train_dl_cnn,  test_dl_cnn = create_dataloaders(\n        train_df, test_df, tok,\n        bs=BATCH_SIZE, max_len=128,\n        use_evidence=False                           # claim-only\n    )\n\n    # BERT ----------------------------------------------------\n    opt_bert, EPOCHS_BERT = AdamW(bert.parameters(), lr=LEARNING_RATE), 3\n    if not force_bert_retrain and os.path.exists(\"bert_model.pt\"):\n        bert.load_state_dict(torch.load(\"bert_model.pt\", map_location=DEVICE))\n        logger.info(\"BERT weights loaded\")\n    else:\n        from sklearn.utils.class_weight import compute_class_weight\n        cw = compute_class_weight(\"balanced\", classes=[0,1,2],\n                                  y=train_df[\"label\"].map(LABEL2ID))\n        train_bert_model(\n            bert, train_dl_bert, test_dl_bert, opt_bert,\n            epochs=EPOCHS_BERT,\n            class_w=torch.tensor(cw, dtype=torch.float, device=DEVICE)\n        )\n        torch.save(bert.state_dict(), \"bert_model.pt\")\n    bert_metrics = evaluate_bert_model(bert, test_dl_bert)\n    plot_metrics(bert_metrics, \"BERT\", show_plots)\n\n    # CNN -----------------------------------------------------\n    bert_emb = bert.get_input_embeddings().weight.data.clone()\n    cnn = CNNModel(\n        tok.vocab_size, bert_emb.size(1), 3,\n        dropout=0.3, pretrained_weights=bert_emb,\n        freeze_embed=True\n    ).to(DEVICE)\n    for block in cnn.convs: block[2].momentum = 0.05\n\n    opt_cnn, EPOCHS_CNN = Adam(cnn.parameters(), lr=LEARNING_RATE), 5\n    if not force_cnn_retrain and os.path.exists(\"cnn_model.pt\"):\n        cnn.load_state_dict(torch.load(\"cnn_model.pt\", map_location=DEVICE))\n        logger.info(\"CNN weights loaded\")\n    else:\n        from sklearn.utils.class_weight import compute_class_weight\n        cw = compute_class_weight(\"balanced\", classes=[0,1,2],\n                                  y=train_df[\"label\"].map(LABEL2ID))\n        train_cnn_model(\n            cnn, train_dl_cnn, test_dl_cnn, opt_cnn,\n            epochs=EPOCHS_CNN,\n            class_w=torch.tensor(cw, dtype=torch.float, device=DEVICE)\n        )\n        torch.save(cnn.state_dict(), \"cnn_model.pt\")\n    cnn_metrics = evaluate_cnn_model(cnn, test_dl_cnn)\n    plot_metrics(cnn_metrics, \"CNN\", show_plots)\n\n    # BM25 baseline (unchanged)\n    k1, b = tune_bm25(train_df, test_df[\"statement\"], test_df[\"label\"],\n                      tune=force_bm25_tune)\n    bm25 = BM25Okapi([preprocess(t) for t in train_df[\"statement\"]], k1=k1, b=b)\n    bm25_metrics = evaluate_bm25(\n        bm25, test_df[\"statement\"], test_df[\"label\"], train_df\n    )\n    plot_metrics(bm25_metrics, \"BM25\", show_plots)\n\n    # summary\n    summary = pd.DataFrame(\n        {\"BERT\": bert_metrics, \"CNN\": cnn_metrics, \"BM25\": bm25_metrics}\n    ).T.round(3)\n    print(\"\\n===== Performance summary =====\\n\", summary)\n\n    return bert, cnn, bm25, tok, train_df, test_df\n\n\nbert_model, cnn_model, bm25, tokenizer, train_df, test_df = main(\n    force_bm25_tune=False,\n    force_bert_retrain=True,\n    force_cnn_retrain=True,\n    fast_dev=True,\n    show_plots=False\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-11T08:27:52.704289Z","iopub.execute_input":"2025-06-11T08:27:52.704463Z","iopub.status.idle":"2025-06-11T08:32:56.690581Z","shell.execute_reply.started":"2025-06-11T08:27:52.704450Z","shell.execute_reply":"2025-06-11T08:32:56.689616Z"}},"outputs":[{"name":"stdout","text":"Deleted old bert_model.pt\nDeleted old cnn_model.pt\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_126/761032842.py:88: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n  .apply(lambda x: x.sample(min(n, len(x)), random_state=42))\n/tmp/ipykernel_126/761032842.py:88: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n  .apply(lambda x: x.sample(min(n, len(x)), random_state=42))\nSome weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"\n===== Performance summary =====\n       Accuracy  Precision  Recall  F1 Score\nBERT     0.476      0.492   0.476     0.470\nCNN      0.450      0.452   0.450     0.437\nBM25     0.396      0.397   0.396     0.396\n","output_type":"stream"}],"execution_count":89},{"cell_type":"code","source":"# Cell 7 – 100 % BM25-free, OOF features, logistic meta-classifier\nimport warnings, numpy as np, torch\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import classification_report\nwarnings.filterwarnings(\"ignore\", category=UserWarning)\n\nLABEL2ID   = {\"SUPPORTS\":0, \"REFUTES\":1, \"NOT ENOUGH INFO\":2}\nFEVER      = [\"SUPPORTS\",\"REFUTES\",\"NOT ENOUGH INFO\"]\nDEVICE     = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# ── helper: base-model softmax\ndef _probs(model, tok, text):\n    enc = tok(text, return_tensors=\"pt\", truncation=True,\n              padding=\"max_length\", max_length=MAX_LENGTH).to(DEVICE)\n    with torch.no_grad():\n        logits = (model(**enc).logits\n                  if hasattr(model, \"config\") else model(enc[\"input_ids\"]))\n    return torch.softmax(logits, 1).squeeze().cpu().numpy()\n\ndef _sharpen(p, g): p = p**g; return p/p.sum()\n\nBERT_G = 1.4   # sharpen BERT only\nCNN_G  = 1.0   # leave CNN raw\nALPHA, BETA = 0.75, 0.25\n\ndef _feat(text):\n    p_bert = _sharpen(_probs(bert_model, tokenizer, text), BERT_G)\n    p_cnn  = _probs  (cnn_model , tokenizer, text)        # no sharpening\n    return np.concatenate([ALPHA*p_bert, BETA*p_cnn])     # 6-dim vector\n\n# ── create OOF training features on the full train set (5-fold) ──────────\nskf   = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\noof_X, oof_y = [], []\n\nfor tr_idx, val_idx in skf.split(train_df, train_df[\"label\"]):\n    for i in val_idx:\n        txt = train_df.iloc[i][\"statement\"]\n        oof_X.append(_feat(txt))\n        oof_y.append(LABEL2ID[train_df.iloc[i][\"label\"]])\n\nX_train = np.vstack(oof_X)\ny_train = np.array(oof_y)\n\n# ── build meta-test features from held-out 40 % slice ────────────────────\nfrom sklearn.model_selection import train_test_split\nmeta_train_df, meta_test_df = train_test_split(\n    test_df, test_size=0.40, stratify=test_df[\"label\"], random_state=42\n)\n\nX_test  = np.vstack([_feat(t) for t in meta_test_df[\"statement\"]])\ny_test  = meta_test_df[\"label\"].map(LABEL2ID).values\n\n# ── logistic regression meta-classifier (simple & robust) ────────────────\nmeta_clf = LogisticRegression(\n    max_iter=2000, class_weight=\"balanced\",\n    multi_class=\"multinomial\", solver=\"lbfgs\"\n)\nmeta_clf.fit(X_train, y_train)\ny_pred = meta_clf.predict(X_test)\n\nprint(\"Stacking Meta-Classifier Report\")\nprint(classification_report(\n    y_test, y_pred, labels=[0,1,2], target_names=FEVER,\n    digits=3, zero_division=0))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-11T08:32:56.692167Z","iopub.execute_input":"2025-06-11T08:32:56.692436Z","iopub.status.idle":"2025-06-11T08:34:18.439449Z","shell.execute_reply.started":"2025-06-11T08:32:56.692412Z","shell.execute_reply":"2025-06-11T08:34:18.438652Z"}},"outputs":[{"name":"stdout","text":"Stacking Meta-Classifier Report\n                 precision    recall  f1-score   support\n\n       SUPPORTS      0.500     0.567     0.531        67\n        REFUTES      0.683     0.642     0.662        67\nNOT ENOUGH INFO      0.475     0.439     0.457        66\n\n       accuracy                          0.550       200\n      macro avg      0.553     0.549     0.550       200\n   weighted avg      0.553     0.550     0.550       200\n\n","output_type":"stream"}],"execution_count":90}]}